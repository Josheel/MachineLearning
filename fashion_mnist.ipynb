{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d052ceab",
   "metadata": {},
   "source": [
    "# Fashion Items Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4185235",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b25e3b",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00c6c8",
   "metadata": {},
   "source": [
    "### Preprocessing the Training and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336275bd",
   "metadata": {},
   "source": [
    "Here we are loading the 'fashion_mnist' dataset accessible through the 'tensorflow_datasets' library. \n",
    "Our call to tfds.load() returns two Tuples, the first is stored into 'dataset' and contains two dictionaries that contain the labled Training and Test data.\n",
    "\n",
    "A detailed overview of he 'tfds.load' class can be found at https://www.tensorflow.org/datasets/api_docs/python/tfds/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0caf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,metadata =tfds.load('fashion_mnist',as_supervised=True,with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d4f82",
   "metadata": {},
   "source": [
    "If we print out the contents of dataset you can see two datasets labled 'test' and 'train'. Both have 'shape' key with a value (28, 28, 1) indicating that our images are all 28 x 28, and greyscale images as indicated by the 1, representing a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a28c45",
   "metadata": {},
   "source": [
    "Inside metadata we have detailed information about our datasets. Lower down you can see a key called 'splits', this is where we hold a count of how many testing and training images we have.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6387195",
   "metadata": {},
   "source": [
    "From 'dataset' we extract the traning and testing data and store them into their on separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d2f22",
   "metadata": {},
   "source": [
    "There are 10 different categories of items, but they are labled numerically from 0 - 9. If we don't keep track of which number represents each item, we won't be able to distinguish which image is what or understand the NN's predictions without directly observing the image. This is why the documentation includes the correct categorical labels represented by each number. In our code we declare an array 'class_names' initialized with the names of all labels, ordered corresponding to the element matching the numerical label. \n",
    "\n",
    "On lines 3 and 4 we extract how many images are in each dataset from the metadata, then print then print them out on lines 5 and 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33452d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "\n",
    "num_train_examples = metadata.splits['train'].num_examples\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "\n",
    "print(\"Training examples = {}\".format(num_train_examples))\n",
    "print(\"Test examples = {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c9974",
   "metadata": {},
   "source": [
    "### Implementing a normalize function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf669bc",
   "metadata": {},
   "source": [
    "Each pixel in each image is color represented by a number in the range 0 - 255. To avoid distortion in our results, we convert all of our attributes to a common range of 0 - 1. The purpose of our function here is to convert the value of each pixel to fit in this reduced range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize (images, label):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images /= 255\n",
    "    return images, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960da22",
   "metadata": {},
   "source": [
    "### Normalizing our datasets and then loading the data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439259b",
   "metadata": {},
   "source": [
    "On Lines 1 and 2 we iterate through the dataset and apply the normalize function on each item.\n",
    "\n",
    "On Lines 4 and 5 we load our data into cache memory to significantly increase the speed with which we can access our data, since we will be accessing it incessantly throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = train_dataset.map(normalize)\n",
    "test_dataset = test_dataset.map(normalize)\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "test_dataset = test_dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65624265",
   "metadata": {},
   "source": [
    "## Part 2: Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaf0a4",
   "metadata": {},
   "source": [
    "### Building the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66982378",
   "metadata": {},
   "source": [
    "Note that all keras.layers classes have many attributes that are all assigned default values. When we create our instances of each, we pass arguments for only the attributes that we need to change based on our desired outcome. For each class we use there will be a link to the documentation for a detailed overview of these classes.\n",
    "\n",
    "#### layer0\n",
    "\n",
    "We change the following attributes: Conv2D(filters, strides, padding, activation, input_shape) ---> 2D Convolution\n",
    "\n",
    "\n",
    "\n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "#### layer1\n",
    "\n",
    "MaxPooling2D(pool_size, strides)\n",
    "\n",
    "https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "\n",
    "#### layer2\n",
    "\n",
    "Conv2D(filters, strides, padding, activation)\n",
    "\n",
    "#### layer3\n",
    "\n",
    "#### layer4\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "\n",
    "#### layer5\n",
    "\n",
    "Dense(units, activation)\n",
    "\n",
    "https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "#### layer6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = tf.keras.layers.Conv2D(32,(3,3),padding='same',activation=tf.nn.relu,input_shape=(28,28,1))\n",
    "layer1 = tf.keras.layers.MaxPooling2D((2,2),strides=2)\n",
    "layer2 = tf.keras.layers.Conv2D(64,(3,3),padding='same',activation=tf.nn.relu)\n",
    "layer3 = tf.keras.layers.MaxPooling2D((2,2),strides=2)\n",
    "layer4 = tf.keras.layers.Flatten() \n",
    "layer5 = tf.keras.layers.Dense(128,activation = tf.nn.relu)\n",
    "layer6 = tf.keras.layers.Dense(10,activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63f6dd",
   "metadata": {},
   "source": [
    "### Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c68c3e",
   "metadata": {},
   "source": [
    "The models are passed to the Sequential API in order from input layer to output layer.\n",
    "\n",
    "https://keras.io/api/models/sequential/\n",
    "\n",
    "https://keras.io/api/models/model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([layer0,layer1,layer2,layer3,layer4,layer5,layer6])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',loss =tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d899655",
   "metadata": {},
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa22c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "train_dataset =train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset= test_dataset.cache().batch(BATCH_SIZE)\n",
    "\n",
    "model.fit(train_dataset,epochs=4,steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))\n",
    "\n",
    "print(\"\\n-------- Training is complete ---------\\n\")\n",
    "\n",
    "test_loss,test_accuracy = model.evaluate(test_dataset,steps=math.ceil(num_test_examples/BATCH_SIZE))\n",
    "\n",
    "print(\"\\nAccuracy ={}\\n\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff2202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
